# LexOrigin Environment Configuration
# Copy this file to .env and customize as needed

# ===========================================
# Ollama LLM Configuration
# ===========================================

# The Ollama model to use for analysis
# Make sure this model is installed: ollama pull <model_name>
OLLAMA_MODEL=gpt-oss:120b-cloud

# Ollama API base URL
# Default is localhost, change if running Ollama on a different host
OLLAMA_BASE_URL=http://localhost:11434

# ===========================================
# Database Configuration
# ===========================================

# ChromaDB persistence directory
# This is where the vector database stores its data
CHROMA_PERSIST_DIR=./api/database

# ===========================================
# Data Ingestion
# ===========================================

# Set to "true" to force re-ingestion of all data on server startup
# Useful when you've updated the data files
LEXORIGIN_FORCE_REFRESH=false

# ===========================================
# Server Configuration
# ===========================================

# Backend API port (default: 8001)
API_PORT=8001

# Frontend development server port (default: 3000)
FRONTEND_PORT=3000

# ===========================================
# Optional: Gemini API (for alternative AI features)
# ===========================================

# Google Gemini API key (optional, for experimental features)
# GEMINI_API_KEY=your_gemini_api_key_here

